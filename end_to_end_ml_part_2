# -*- coding: utf-8 -*-
"""
Created on Wed Feb 26 15:05:31 2020

"""
import os
import tarfile
from six.moves import urllib
'''
# Fetch the data
DOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml/master/"
print(DOWNLOAD_ROOT)
HOUSING_PATH = 'datasets/housing'
HOUSING_URL = DOWNLOAD_ROOT + HOUSING_PATH + '/housing.tgz'

def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):
    if not os.path.isdir(housing_path):
        os.makedirs(housing_path)
    tgz_path = os.path.join(housing_path, 'housing.tgz')
    urllib.request.urlretrieve(housing_url, tgz_path)
    housing_tgz = tarfile.open(tgz_path)
    housing_tgz.extractall(path=housing_path)
    housing_tgz.close()

# Write a small function to load the data
import pandas as pd
def load_housing_data(housing_path=HOUSING_PATH):
    csv_path = os.path.join(housing_path, "housing.csv")
    return pd.read_csv(csv_path)

fetch_housing_data()
housing = load_housing_data()
# The url does not work properly.
''' 
'''
tar = tarfile.open("cal_housing.tgz")
for tarinfo in tar:
    print(tarinfo.name, "is", tarinfo.size, "bytes in size and is", end="")
    if tarinfo.isreg():
        print("a regular file.")
    elif tarinfo.isdir():
        print("a directory.")
    else:
        print("something else.")
tar.close()
'''
'''
# Test to understand np.ceil() & where() & inplace
df = np.array([1.0, 0.2, 0.5, 1.7, 2.3, 2.9, 5.5, 7.2])
q = pd.Series(np.ceil(df))
print(q)
q.where(q<=8, 4.0, inplace=True)
print(q)
'''

# The first way to load the data, what is the difference between these 2 ways of loading data?
import pandas as pd
# housing = pd.read_csv('cal_housing.tgz', compression='gzip', header=0, sep=' ', quotechar='"', error_bad_lines=False)

# The second way to load the data
with tarfile.open("cal_housing.tgz") as tar:
    csv_path = tar.getnames()[0]
    housing = pd.read_csv((tar.extractfile(csv_path)), names=["longitude", "latitude", "housing_median_age", "total_rooms", "total_bedrooms", "population", "households", "median_income", "median_house_value"])
    
print(housing.head())
print(housing.info())
print(housing.describe())
print(type(housing))

import matplotlib.pyplot as plt
housing.hist(bins=50, figsize=(20, 15))
plt.show()

'''
# it is fine to use the following random sampling method to split the train and test sets if your dataset is large enough. But...
from sklearn.model_selection import train_test_split
train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)
print(test_set)
'''

# but if you do not have large dataset, you run the risk of introducing a significant sampling bias. The test set generated using stratified sampling has
# income category proportions almost identical to those in the full dataset, whereas the test set generated using purely random sampling is quite skewed.
import numpy as np
housing['income_cat'] = np.ceil(housing['median_income'] / 1.5)
housing['income_cat'].where(housing['income_cat'] < 5, 5.0, inplace=True)

from sklearn.model_selection import StratifiedShuffleSplit
split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_index, test_index in split.split(housing, housing["income_cat"]):
    strat_train_set = housing.loc[train_index]
    strat_test_set = housing.loc[test_index]

print(housing['income_cat'].value_counts() / len(housing))

for set in (strat_train_set, strat_test_set):
    set.drop(["income_cat"], axis=1, inplace=True)

# Now we can move on to the next stage: exploring the data
# Create a copy of the training set so you can play without harming it
housing = strat_train_set.copy()
housing.plot(kind="scatter", x="longitude", y="latitude")    
# Setting the alpha option to 0.1 makes it much easier to visualize the places where there is a high density of data points
housing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.1)

housing.plot(kind="scatter", x="longitude", y="latitude", alpha=0.4,
s=housing["population"]/100, label="population",
c="median_house_value", cmap=plt.get_cmap("jet"), colorbar=True,
)
plt.legend()

# Looking for Correlations
corr_matrix = housing.corr()
corr_matrix['median_house_value'].sort_values(ascending=False)
print(corr_matrix)



